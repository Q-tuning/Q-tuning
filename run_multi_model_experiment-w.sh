#!/bin/bash

# ========================================
# Multi-Model Data Token Pruning å®Œæ•´å®éªŒæ‰§è¡Œè„šæœ¬
# Models: llama2-7b, qwen2-7b, mistral-7b
# Datasets: limr, wizard
# ========================================

bash /mnt/public/wangshaobo/conda_init.sh

echo "ğŸš€ å¼€å§‹Multi-Model Data Token Pruningå®éªŒ"
echo "æ”¯æŒæ¨¡å‹: llama2-7b, qwen2-7b, mistral-7b"
echo "æ”¯æŒæ•°æ®é›†: limr, wizard"
echo "========================================"

export PEFT_DISABLE_HUB_DOWNLOAD=1
export HF_HUB_DISABLE_TELEMETRY=1
export HF_HUB_OFFLINE=1
export TRANSFORMERS_OFFLINE=1
export FORCE_TORCHRUN=1

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
BOLD='\033[1m'
NC='\033[0m' # No Color

# åŸºç¡€è·¯å¾„
BASE="/mnt/public/gpfs-jd/code/wangshaobo/Data_Token_Pruning"
DATA_BASE="/mnt/public/gpfs-jd/data/wangshaobo/Data_Token_Pruning"
MODELS_DIR="${DATA_BASE}/models"
SAVES_DIR="${DATA_BASE}/saves"
CHECKPOINTS_DIR="${DATA_BASE}/checkpoints"
LOG_DIR="${BASE}/experiment_logs"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")

# åˆ›å»ºå¿…è¦ç›®å½•
mkdir -p "$LOG_DIR"
mkdir -p "${BASE}/evaluation_results_summary"
mkdir -p "${BASE}/evaluation_results"

# æ—¥å¿—æ–‡ä»¶
MASTER_LOG="${LOG_DIR}/multi_model_experiment_${TIMESTAMP}.log"
PRUNING_LOG="${LOG_DIR}/multi_model_pruning_${TIMESTAMP}.log"

# æ‰§è¡ŒçŠ¶æ€å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1" | tee -a "$MASTER_LOG"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1" | tee -a "$MASTER_LOG"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1" | tee -a "$MASTER_LOG"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1" | tee -a "$MASTER_LOG"
}

log_stage() {
    echo -e "${BOLD}${BLUE}========================================${NC}" | tee -a "$MASTER_LOG"
    echo -e "${BOLD}${BLUE}$1${NC}" | tee -a "$MASTER_LOG"
    echo -e "${BOLD}${BLUE}========================================${NC}" | tee -a "$MASTER_LOG"
}

# æ£€æŸ¥è„šæœ¬å’Œæ–‡ä»¶æ˜¯å¦å­˜åœ¨
check_dependencies() {
    log_info "æ£€æŸ¥å¿…è¦æ–‡ä»¶å’Œè„šæœ¬æ˜¯å¦å­˜åœ¨..."
    
    local dependencies=(
        "${BASE}/multi_model_pruning-w.sh"
        "${BASE}/multi_model_sft.yaml"
        "${BASE}/gpu_environment_check.sh"
        "${DATA_BASE}/limr.json"
        "${DATA_BASE}/wizard.json"
    )
    
    for dep in "${dependencies[@]}"; do
        if [ -f "$dep" ]; then
            log_success "æ–‡ä»¶å­˜åœ¨: $(basename $dep)"
        else
            log_error "æ–‡ä»¶ä¸å­˜åœ¨: $dep"
            return 1
        fi
    done
    
    # æ£€æŸ¥æ¨¡å‹ç›®å½•
    local models=("llama2-7b" "qwen2-7b" "mistral-7b")
    for model in "${models[@]}"; do
        if [ -d "${MODELS_DIR}/${model}" ]; then
            log_success "æ¨¡å‹å­˜åœ¨: $model"
        else
            log_warning "æ¨¡å‹ä¸å­˜åœ¨: ${MODELS_DIR}/${model}"
        fi
    done
    
    return 0
}

# è¿è¡Œç¯å¢ƒæ£€æŸ¥
run_environment_check() {
    log_stage "é˜¶æ®µ0: GPUç¯å¢ƒæ£€æŸ¥"
    
    log_info "è¿è¡ŒGPUç¯å¢ƒæ£€æŸ¥..."
    # if [ -f "${BASE}/gpu_environment_check.sh" ]; then
    #     bash "${BASE}/gpu_environment_check.sh" 2>&1 | tee -a "$MASTER_LOG"
    #     local exit_code=$?
        
    #     if [ $exit_code -eq 0 ]; then
    #         log_success "GPUç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ"
    #     elif [ $exit_code -eq 1 ]; then
    #         log_warning "GPUç¯å¢ƒæ£€æŸ¥æœ‰è­¦å‘Šï¼Œä½†å¯ä»¥ç»§ç»­æ‰§è¡Œ"
    #     else
    #         log_error "GPUç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·å…ˆä¿®å¤ç¯å¢ƒé—®é¢˜"
    #         return 1
    #     fi
    # else
    #     log_warning "GPUç¯å¢ƒæ£€æŸ¥è„šæœ¬ä¸å­˜åœ¨ï¼Œè·³è¿‡ç¯å¢ƒæ£€æŸ¥"
    # fi
    
    return 0
}

# é€‰æ‹©æ•°æ®é›†
select_dataset() {
    local dataset_choice="$1"
    
    case "$dataset_choice" in
        "limr"|"1")
            export DATASET="limr"
            log_info "é€‰æ‹©æ•°æ®é›†: limr"
            ;;
        "wizard"|"2")
            export DATASET="wizard"  
            log_info "é€‰æ‹©æ•°æ®é›†: wizard"
            ;;
        "alpaca"|"3")
            export DATASET="alpaca"  
            log_info "é€‰æ‹©æ•°æ®é›†: alpaca"
            ;;
        *)
            export DATASET="wizard"  # é»˜è®¤ä½¿ç”¨wizard
            log_warning "æ— æ•ˆçš„æ•°æ®é›†é€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤æ•°æ®é›†: limr"
            ;;
    esac
}

# æ‰§è¡ŒMulti-Modelå‰ªæå®éªŒ
run_multi_model_pruning() {
    log_stage "Multi-Model Data Token Pruning å®éªŒ"

    log_info "å¼€å§‹Multi-Modelå‰ªæå®éªŒ..."
    log_info "æ•°æ®é›†: $DATASET"
    log_info "æ”¯æŒæ¨¡å‹: llama2-7b, qwen2-7b, mistral-7b"
    log_info "æ•°æ®æ–¹æ³•: random, infobatch, longest, entropy"
    log_info "Tokenæ–¹æ³•: random, sample_fastv (ç§»é™¤rho)"
    log_info "æ—¥å¿—æ–‡ä»¶: $PRUNING_LOG"

    # æ ¹æ®å…¨å±€å˜é‡è®¾ç½®ç¯å¢ƒå˜é‡
    if [ "$SKIP_TRAINING" = true ]; then
        export SKIP_TRAINING=true
        log_info "è®¾ç½®: è·³è¿‡è®­ç»ƒ"
    fi
    if [ "$SKIP_PRUNING" = true ]; then
        export SKIP_PRUNING=true
        log_info "è®¾ç½®: è·³è¿‡å‰ªæ"
    fi
    if [ "$SKIP_EVALUATION" = true ]; then
        export SKIP_EVALUATION=true
        log_info "è®¾ç½®: è·³è¿‡è¯„æµ‹"
    fi

    # è®°å½•å¼€å§‹æ—¶é—´
    local start_time=$(date)
    log_info "å‰ªæå®éªŒå¼€å§‹æ—¶é—´: $start_time"

    # æ‰§è¡Œå‰ªæå®éªŒè„šæœ¬
    bash "${BASE}/multi_model_pruning-w.sh" 2>&1 | tee "$PRUNING_LOG"
    local pruning_exit_code=$?

    # è®°å½•ç»“æŸæ—¶é—´
    local end_time=$(date)
    log_info "å‰ªæå®éªŒç»“æŸæ—¶é—´: $end_time"

    if [ $pruning_exit_code -eq 0 ]; then
        log_success "Multi-Modelå‰ªæå®éªŒå®Œæˆ"
        return 0
    else
        log_error "Multi-Modelå‰ªæå®éªŒå¤±è´¥ (é€€å‡ºä»£ç : $pruning_exit_code)"
        return 1
    fi
}

# æ‰§è¡Œä»…è¯„æµ‹ï¼ˆè·³è¿‡è®­ç»ƒå’Œå‰ªæï¼‰
run_evaluation_only() {
    log_stage "Multi-Model ä»…è¯„æµ‹æ¨¡å¼"

    log_info "å¼€å§‹Multi-Modelä»…è¯„æµ‹..."
    log_info "æ•°æ®é›†: $DATASET"
    log_info "ç›´æ¥ä½¿ç”¨ç°æœ‰æ¨¡å‹è¿›è¡Œè¯„æµ‹"
    log_info "æ—¥å¿—æ–‡ä»¶: $PRUNING_LOG"

    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼šè·³è¿‡è®­ç»ƒå’Œå‰ªæï¼Œä½†æ‰§è¡Œè¯„æµ‹
    export SKIP_TRAINING=true
    export SKIP_PRUNING=true  
    export SKIP_EVALUATION=false

    # è®°å½•å¼€å§‹æ—¶é—´
    local start_time=$(date)
    log_info "è¯„æµ‹å¼€å§‹æ—¶é—´: $start_time"

    # æ‰§è¡Œè¯„æµ‹è„šæœ¬
    bash "${BASE}/multi_model_pruning-w.sh" 2>&1 | tee "$PRUNING_LOG"
    local eval_exit_code=$?

    # è®°å½•ç»“æŸæ—¶é—´
    local end_time=$(date)
    log_info "è¯„æµ‹ç»“æŸæ—¶é—´: $end_time"

    if [ $eval_exit_code -eq 0 ]; then
        log_success "Multi-Modelä»…è¯„æµ‹å®Œæˆ"
        return 0
    else
        log_error "Multi-Modelä»…è¯„æµ‹å¤±è´¥ (é€€å‡ºä»£ç : $eval_exit_code)"
        return 1
    fi
}

# ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
generate_final_report() {
    log_stage "ç”Ÿæˆæœ€ç»ˆå®éªŒæŠ¥å‘Š"
    
    # ä»è„šæœ¬å†™å‡ºçš„å…ƒæ•°æ®ä¸­è¯»å–æ—¶é—´æˆ³ä¸ç»“æœè·¯å¾„
    local META_FILE="${LOG_DIR}/multi_model_last_run.env"
    local MULTI_TS="$TIMESTAMP"
    local report_file
    local csv_results
    local markdown_results

    if [ -f "$META_FILE" ]; then
        log_info "æ£€æµ‹åˆ°è¯„æµ‹å…ƒæ•°æ®: $META_FILE"
        source "$META_FILE"
        if [ -n "$TIME_STAMP" ]; then
            MULTI_TS="$TIME_STAMP"
        fi
        if [ -n "$RESULTS_SUMMARY_FILE" ]; then
            csv_results="$RESULTS_SUMMARY_FILE"
        else
            csv_results="${BASE}/evaluation_results_summary/evaluation_results_summary_${MULTI_TS}.csv"
        fi
        if [ -n "$MARKDOWN_TABLE" ]; then
            markdown_results="$MARKDOWN_TABLE"
        else
            markdown_results="${BASE}/evaluation_results/evaluation_results_${MULTI_TS}.md"
        fi
    else
        log_warning "æœªå‘ç°è¯„æµ‹å…ƒæ•°æ®æ–‡ä»¶ï¼Œå›é€€ä½¿ç”¨å½“å‰æ—¶é—´æˆ³"
        csv_results="${BASE}/evaluation_results_summary/evaluation_results_summary_${TIMESTAMP}.csv"
        markdown_results="${BASE}/evaluation_results/evaluation_results_${TIMESTAMP}.md"
    fi

    report_file="${LOG_DIR}/multi_model_experiment_report_${MULTI_TS}.md"
    
    {
        echo "# Multi-Model Data Token Pruning å®éªŒæŠ¥å‘Š"
        echo ""
        echo "**å®éªŒæ—¶é—´:** $(date)"
        echo "**å®éªŒæ—¶é—´æˆ³:** $MULTI_TS"
        echo "**æ•°æ®é›†:** ${DATASET:-limr}"
        echo ""
        echo "## å®éªŒé…ç½®"
        echo "- **æ”¯æŒæ¨¡å‹:** llama2-7b, qwen2-7b, mistral-7b"
        echo "- **æ•°æ®é›†:** ${DATASET:-limr} (limr.json æˆ– wizard.json)"
        echo "- **æ•°æ®å‰ªææ–¹æ³•:** random, infobatch, longest, entropy"
        echo "- **Tokenå‰ªææ–¹æ³•:** random, sample_fastv (ç§»é™¤rho)"
        echo "- **æ•°æ®æ¯”ä¾‹:** 30%, 50%, 70%"
        echo "- **Tokenæ¯”ä¾‹:** 30%, 50%, 70%"
        echo ""
        echo "## ä¸»è¦æ”¹è¿›"
        echo "1. **ç®€åŒ–æ¶æ„:** ç§»é™¤reference_modelï¼Œç®€åŒ–è·¯å¾„ç»“æ„"
        echo "2. **å¤šæ¨¡å‹æ”¯æŒ:** æ”¯æŒllama2, qwen2, mistralä¸‰ç§æ¨¡å‹æ¶æ„"
        echo "3. **æ–°æ•°æ®é›†:** ä½¿ç”¨limr.jsonå’Œwizard.jsonæ›¿ä»£alpaca"
        echo "4. **ç²¾ç®€tokenæ–¹æ³•:** ç§»é™¤rhoæ–¹æ³•ï¼Œä¸“æ³¨äºrandomå’Œsample_fastv"
        echo ""
        echo "## å®éªŒæµç¨‹"
        echo "1. **æ•°æ®é¢„å¤„ç†:** æ ¹æ®é€‰æ‹©çš„æ•°æ®é›†åŠ è½½å¯¹åº”çš„JSONæ–‡ä»¶"
        echo "2. **æ¨¡å‹è®­ç»ƒ:** ä½¿ç”¨LoRAå¾®è°ƒå„ä¸ªåŸºç¡€æ¨¡å‹"
        echo "3. **æ¨¡å‹å‰ªæ:** åº”ç”¨æ•°æ®å’Œtokenå‰ªæç­–ç•¥"
        echo "4. **è¯„æµ‹:** OpenCompasså¤šæ•°æ®é›†è¯„æµ‹"
        echo ""
        echo "## æ—¥å¿—æ–‡ä»¶"
        echo "- **ä¸»æ—¥å¿—:** $MASTER_LOG"
        echo "- **å‰ªæå®éªŒæ—¥å¿—:** $PRUNING_LOG"
        echo ""
        echo "## è¯„æµ‹ç»“æœ"
        
        if [ -f "$markdown_results" ]; then
            echo "è¯¦ç»†è¯„æµ‹ç»“æœè¯·æŸ¥çœ‹: $markdown_results"
            echo ""
            echo "### ç»“æœæ‘˜è¦"
            cat "$markdown_results" | tail -n +3
        else
            echo "è¯„æµ‹ç»“æœæ–‡ä»¶æœªæ‰¾åˆ°: $markdown_results"
        fi
        
        echo ""
        echo "## ç»“æœæ–‡ä»¶ä½ç½®"
        echo "- **CSVç»“æœ:** $csv_results"
        echo "- **Markdownç»“æœ:** $markdown_results"
        echo "- **æ¨¡å‹è¾“å‡º:** ${CHECKPOINTS_DIR}/"
        echo "- **è®­ç»ƒè¾“å‡º:** ${SAVES_DIR}/"
        echo ""
        echo "## è·¯å¾„ç»“æ„å˜åŒ–"
        echo "**æ—§ç»“æ„:** \`\${SAVE_DIR}/\${DATASET}/\${MODEL}/\${REFERENCE_MODEL}/...\`"
        echo "**æ–°ç»“æ„:** \`\${SAVE_DIR}/\${DATASET}/\${MODEL}/...\`"
        echo ""
        echo "ç§»é™¤äº†reference_modelå±‚çº§ï¼Œç®€åŒ–äº†ç›®å½•ç»“æ„ã€‚"
    } > "$report_file"
    
    log_success "æœ€ç»ˆå®éªŒæŠ¥å‘Šå·²ç”Ÿæˆ: $report_file"
}

# æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
show_usage() {
    echo "ç”¨æ³•: $0 [æ¨¡å¼] [æ•°æ®é›†]"
    echo ""
    echo "æ¨¡å¼é€‰é¡¹:"
    echo "  1 - å®Œæ•´å®éªŒ (è®­ç»ƒ + å‰ªæ + è¯„æµ‹)"
    echo "  2 - ä»…è¯„æµ‹ (ä½¿ç”¨ç°æœ‰æ¨¡å‹)"
    echo "  3 - ä»…è®­ç»ƒå’Œå‰ªæ (è·³è¿‡è¯„æµ‹)"
    echo ""
    echo "æ•°æ®é›†é€‰é¡¹:"
    echo "  limr/1 - ä½¿ç”¨limr.jsonæ•°æ®é›†"
    echo "  wizard/2 - ä½¿ç”¨wizard.jsonæ•°æ®é›†"
    echo ""
    echo "ç¤ºä¾‹:"
    echo "  $0 1 limr    # å®Œæ•´å®éªŒï¼Œä½¿ç”¨limræ•°æ®é›†"
    echo "  $0 2 wizard  # ä»…è¯„æµ‹ï¼Œä½¿ç”¨wizardæ•°æ®é›†"
    echo ""
}

# ä¸»æ‰§è¡Œå‡½æ•°
main() {
    # è®°å½•å¼€å§‹æ—¶é—´
    local experiment_start_time=$(date)
    log_info "Multi-Modelå®éªŒå¼€å§‹æ—¶é—´: $experiment_start_time"

    # è§£æå‚æ•°
    local mode_choice=${1:-1}
    local dataset_choice=${2:-limr}
    
    # å¦‚æœæ²¡æœ‰æä¾›å‚æ•°ï¼Œæ˜¾ç¤ºäº¤äº’å¼é€‰æ‹©
    if [ $# -eq 0 ]; then
        echo "è¯·é€‰æ‹©å®éªŒæ¨¡å¼:"
        echo "  1) å®Œæ•´å®éªŒ (è®­ç»ƒ + å‰ªæ + è¯„æµ‹)"
        echo "  2) ä»…è¯„æµ‹ (ä½¿ç”¨ç°æœ‰æ¨¡å‹)"
        echo "  3) ä»…è®­ç»ƒå’Œå‰ªæ (è·³è¿‡è¯„æµ‹)"
        echo ""
        read -p "è¯·è¾“å…¥é€‰æ‹© (1/2/3): " mode_choice
        
        echo ""
        echo "è¯·é€‰æ‹©æ•°æ®é›†:"
        echo "  1) limr (limr.json)"
        echo "  2) wizard (wizard.json)"
        echo ""
        read -p "è¯·è¾“å…¥é€‰æ‹© (1/2): " dataset_choice
    fi

    # é€‰æ‹©æ•°æ®é›†
    select_dataset "$dataset_choice"

    # ç”¨æˆ·é€‰æ‹©æ¨¡å¼
    log_stage "å®éªŒæ¨¡å¼é…ç½®"
    local SKIP_TRAINING=false
    local SKIP_PRUNING=false
    local SKIP_EVALUATION=false

    case "$mode_choice" in
        1)
            log_info "é€‰æ‹©ï¼šå®Œæ•´å®éªŒæ¨¡å¼ (è®­ç»ƒ + å‰ªæ + è¯„æµ‹)"
            SKIP_TRAINING=false
            SKIP_PRUNING=false
            SKIP_EVALUATION=false
            ;;
        2)
            log_info "é€‰æ‹©ï¼šä»…è¯„æµ‹æ¨¡å¼"
            SKIP_TRAINING=true
            SKIP_PRUNING=true
            SKIP_EVALUATION=false
            ;;
        3)
            log_info "é€‰æ‹©ï¼šä»…è®­ç»ƒå’Œå‰ªææ¨¡å¼"
            SKIP_TRAINING=false
            SKIP_PRUNING=false
            SKIP_EVALUATION=true
            ;;
        *)
            log_warning "æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤å®Œæ•´å®éªŒæ¨¡å¼"
            SKIP_TRAINING=false
            SKIP_PRUNING=false
            SKIP_EVALUATION=false
            ;;
    esac

    echo ""

    # æ£€æŸ¥ä¾èµ–
    if ! check_dependencies; then
        log_error "ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œé€€å‡º"
        exit 1
    fi

    # ç¯å¢ƒæ£€æŸ¥
    if ! run_environment_check; then
        log_error "ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œé€€å‡º"
        exit 1
    fi

    # è®¾ç½®å…¨å±€ç¯å¢ƒå˜é‡
    export SKIP_TRAINING
    export SKIP_PRUNING  
    export SKIP_EVALUATION

    # æ‰§è¡Œå®éªŒ
    if [ "$SKIP_TRAINING" = true ] && [ "$SKIP_PRUNING" = true ]; then
        # ä»…è¯„æµ‹æ¨¡å¼
        if ! run_evaluation_only; then
            log_error "ä»…è¯„æµ‹å¤±è´¥"
            exit 1
        fi
    else
        # å®Œæ•´å®éªŒæˆ–ä»…è®­ç»ƒå‰ªææ¨¡å¼
        if ! run_multi_model_pruning; then
            log_error "Multi-Modelå‰ªæå®éªŒå¤±è´¥"
            exit 1
        fi
    fi

    # ç”ŸæˆæŠ¥å‘Š
    generate_final_report
    
    # è®°å½•å®Œæˆæ—¶é—´
    local experiment_end_time=$(date)
    log_info "Multi-Modelå®éªŒç»“æŸæ—¶é—´: $experiment_end_time"
    
    log_stage "ğŸ‰ Multi-Modelå®éªŒæ‰§è¡Œå®Œæˆï¼"

    echo ""
    echo "å®éªŒæ€»ç»“:"
    echo "  âœ… æ•°æ®é›†: ${DATASET}"
    echo "  âœ… æ”¯æŒæ¨¡å‹: llama2-7b, qwen2-7b, mistral-7b"
    
    if [ "$SKIP_TRAINING" = false ]; then
        echo "  âœ… è®­ç»ƒ: LoRAå¾®è°ƒå„ä¸ªåŸºç¡€æ¨¡å‹"
    else
        echo "  â­ï¸  è·³è¿‡è®­ç»ƒé˜¶æ®µ"
    fi

    if [ "$SKIP_PRUNING" = false ]; then
        echo "  âœ… å‰ªæ: æ•°æ®å’ŒTokenå‰ªæå®éªŒ"
    else
        echo "  â­ï¸  è·³è¿‡å‰ªæé˜¶æ®µ"
    fi
    
    if [ "$SKIP_EVALUATION" = false ]; then
        echo "  âœ… è¯„æµ‹: OpenCompasså¤šæ•°æ®é›†è¯„æµ‹"
    else
        echo "  â­ï¸  è·³è¿‡è¯„æµ‹é˜¶æ®µ"
    fi

    echo "  âœ… ç»“æœæ±‡æ€»: è‡ªåŠ¨ç”Ÿæˆè¯„æµ‹ç»“æœè¡¨æ ¼"
    echo ""

    echo "ç»“æœæ–‡ä»¶:"
    local final_ts="${TIME_STAMP:-$TIMESTAMP}"
    echo "  ğŸ“Š è¯„æµ‹ç»“æœ: ${BASE}/evaluation_results/evaluation_results_${final_ts}.md"
    echo "  ğŸ“ˆ CSVæ•°æ®: ${BASE}/evaluation_results_summary/evaluation_results_summary_${final_ts}.csv"
    echo "  ğŸ“ å®éªŒæŠ¥å‘Š: ${LOG_DIR}/multi_model_experiment_report_${final_ts}.md"
    echo ""

    echo "å¦‚éœ€é‡æ–°è¿è¡Œä»»ä½•é˜¶æ®µï¼Œè¯·æŸ¥çœ‹å¯¹åº”çš„æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†ä¿¡æ¯ã€‚"
    echo "ä½¿ç”¨ '$0 --help' æŸ¥çœ‹è¯¦ç»†ä½¿ç”¨è¯´æ˜ã€‚"
}

# æ£€æŸ¥å¸®åŠ©å‚æ•°
if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
    show_usage
    exit 0
fi

# è„šæœ¬å¼€å§‹æ‰§è¡Œ
echo "æ—¥å¿—å°†ä¿å­˜åˆ°: $MASTER_LOG"
echo ""

# æ•è·ä¸­æ–­ä¿¡å·
trap 'log_error "å®éªŒè¢«ç”¨æˆ·ä¸­æ–­"; exit 130' INT

# æ‰§è¡Œä¸»å‡½æ•°
export WANDB_MODE=offline
export WANDB_DIR=${BASE}/wandb
mkdir -p "$WANDB_DIR"

main "$@"
